{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-18T10:57:05.841770Z",
     "iopub.status.busy": "2024-08-18T10:57:05.841398Z",
     "iopub.status.idle": "2024-08-18T10:57:23.047545Z",
     "shell.execute_reply": "2024-08-18T10:57:23.046559Z",
     "shell.execute_reply.started": "2024-08-18T10:57:05.841738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as v2\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import ViTConfig, ViTImageProcessor, ViTModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T10:57:23.050344Z",
     "iopub.status.busy": "2024-08-18T10:57:23.049533Z",
     "iopub.status.idle": "2024-08-18T10:57:23.084127Z",
     "shell.execute_reply": "2024-08-18T10:57:23.082994Z",
     "shell.execute_reply.started": "2024-08-18T10:57:23.050306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T12:27:04.565459Z",
     "iopub.status.busy": "2024-08-18T12:27:04.564998Z",
     "iopub.status.idle": "2024-08-18T12:27:04.573340Z",
     "shell.execute_reply": "2024-08-18T12:27:04.572320Z",
     "shell.execute_reply.started": "2024-08-18T12:27:04.565422Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "labels = ['bag', 'bottom', 'dress', 'hat', 'outer', 'shoes', 'top', 'etc']\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T10:57:23.123487Z",
     "iopub.status.busy": "2024-08-18T10:57:23.123150Z",
     "iopub.status.idle": "2024-08-18T10:57:23.600428Z",
     "shell.execute_reply": "2024-08-18T10:57:23.599480Z",
     "shell.execute_reply.started": "2024-08-18T10:57:23.123456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ckpt = 'google/vit-base-patch16-224-in21k'\n",
    "config = ViTConfig.from_pretrained(ckpt)\n",
    "image_processor = ViTImageProcessor.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T10:57:23.602248Z",
     "iopub.status.busy": "2024-08-18T10:57:23.601808Z",
     "iopub.status.idle": "2024-08-18T11:02:40.801765Z",
     "shell.execute_reply": "2024-08-18T11:02:40.800904Z",
     "shell.execute_reply.started": "2024-08-18T10:57:23.602215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, config, image_processor):\n",
    "        self.label_ids = []\n",
    "        self.image_paths = []\n",
    "        self.image_processor = image_processor\n",
    "        categories = [d for d in os.listdir(image_dir) if not d.startswith('.')]\n",
    "        for category in categories:\n",
    "            image_fnames = [f for f in os.listdir(f'{image_dir}/{category}') if not f.startswith('.')]\n",
    "            for image_fname in image_fnames:\n",
    "                image_path = f'{image_dir}/{category}/{image_fname}'\n",
    "                label_id = label2id[category] \n",
    "                self.label_ids.append(label_id)\n",
    "                self.image_paths.append(image_path)\n",
    "        \n",
    "        self.transform = v2.Compose([\n",
    "            v2.Resize((config.image_size, config.image_size)),\n",
    "            v2.ToTensor(),\n",
    "            v2.Normalize(mean=self.image_processor.image_mean, std=self.image_processor.image_std),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label_ids)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.image_paths[i]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return self.label_ids[i], image\n",
    "    \n",
    "dataset = CustomDataset('../crawl/kream_thumbnails', config, image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T11:02:40.803007Z",
     "iopub.status.busy": "2024-08-18T11:02:40.802746Z",
     "iopub.status.idle": "2024-08-18T11:02:40.827479Z",
     "shell.execute_reply": "2024-08-18T11:02:40.826481Z",
     "shell.execute_reply.started": "2024-08-18T11:02:40.802985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(2024)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=generator)\n",
    "\n",
    "print(f'train: {len(train_dataset)}, test: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T11:02:40.829033Z",
     "iopub.status.busy": "2024-08-18T11:02:40.828699Z",
     "iopub.status.idle": "2024-08-18T11:02:40.837368Z",
     "shell.execute_reply": "2024-08-18T11:02:40.836511Z",
     "shell.execute_reply.started": "2024-08-18T11:02:40.829009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)\n",
    "val_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T11:02:40.840479Z",
     "iopub.status.busy": "2024-08-18T11:02:40.840171Z",
     "iopub.status.idle": "2024-08-18T11:02:40.846688Z",
     "shell.execute_reply": "2024-08-18T11:02:40.845707Z",
     "shell.execute_reply.started": "2024-08-18T11:02:40.840445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.fc = nn.Linear(config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.fc(self.vit(x).pooler_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T11:26:54.954653Z",
     "iopub.status.busy": "2024-08-18T11:26:54.953816Z",
     "iopub.status.idle": "2024-08-18T11:26:55.500428Z",
     "shell.execute_reply": "2024-08-18T11:26:55.499638Z",
     "shell.execute_reply.started": "2024-08-18T11:26:54.954621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Classifier(num_labels=len(labels)).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T11:26:55.502170Z",
     "iopub.status.busy": "2024-08-18T11:26:55.501819Z",
     "iopub.status.idle": "2024-08-18T11:26:55.512275Z",
     "shell.execute_reply": "2024-08-18T11:26:55.511286Z",
     "shell.execute_reply.started": "2024-08-18T11:26:55.502145Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        labels, images = batch\n",
    "        labels = labels.to(device)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    corrects = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        labels, images = batch\n",
    "        labels = labels.to(device)\n",
    "        images = images.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        \n",
    "        total += len(labels)\n",
    "        corrects += (preds == labels).sum().item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader), corrects / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-18T11:26:55.846151Z",
     "iopub.status.busy": "2024-08-18T11:26:55.845784Z",
     "iopub.status.idle": "2024-08-18T12:17:05.257485Z",
     "shell.execute_reply": "2024-08-18T12:17:05.256471Z",
     "shell.execute_reply.started": "2024-08-18T11:26:55.846123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "min_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "model_save_dir = './model_ckpt'\n",
    "if not os.path.isdir(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "    val_loss, val_accuracy = evaluate(model, val_dataloader, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Train loss{train_loss:.3f}, Eval loss: {val_loss:.3f}, Eval accuracy: {val_accuracy:.3f}')\n",
    "    \n",
    "    if val_loss < min_loss:\n",
    "        torch.save(\n",
    "            copy.deepcopy(model).to(torch.device(\"cpu\")).state_dict(),\n",
    "            f\"{model_save_dir}/classifier.pt\",\n",
    "        )\n",
    "        min_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter == 3:\n",
    "            print(f\"Early stopped at epoch {epoch + 1}\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 139630,
     "sourceId": 329006,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "fashion-visual-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
