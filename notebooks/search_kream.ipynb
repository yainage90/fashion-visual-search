{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForObjectDetection, AutoImageProcessor, SwinModel, SwinConfig\n",
    "\n",
    "from huggingface_hub import PyTorchModelHubMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = 'yainage90/fashion-object-detection'\n",
    "detector_image_processor = AutoImageProcessor.from_pretrained(ckpt)\n",
    "detector = AutoModelForObjectDetection.from_pretrained(ckpt).to(device)\n",
    "\n",
    "ckpt = \"yainage90/fashion-image-feature-extractor\"\n",
    "encoder_config = SwinConfig.from_pretrained(ckpt)\n",
    "encoder_image_processor = AutoImageProcessor.from_pretrained(ckpt)\n",
    "\n",
    "class ImageEncoder(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.swin = SwinModel(config=encoder_config)\n",
    "        self.embedding_layer = nn.Linear(encoder_config.hidden_size, 128)\n",
    "\n",
    "    def forward(self, image_tensor):\n",
    "        features = self.swin(image_tensor).pooler_output\n",
    "        embeddings = self.embedding_layer(features)\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "encoder = ImageEncoder().from_pretrained('yainage90/fashion-image-feature-extractor').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x3, y3, x4, y4 = box2\n",
    "\n",
    "    xx1, yy1 = max(x1, x3), max(y1, y3)\n",
    "    xx2, yy2 = min(x2, x4), min(y2, y4)\n",
    "\n",
    "    intersection_area = max(0, xx2 - xx1) * max(0, yy2 - yy1)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x4 - x3) * (y4 - y3)\n",
    "\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    return iou\n",
    "\n",
    "def non_max_suppression(items, iou_threshold=0.7):\n",
    "    sorted_items = sorted(items, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    keep = []\n",
    "    while sorted_items:\n",
    "        current = sorted_items.pop(0)\n",
    "        keep.append(current)\n",
    "        \n",
    "        sorted_items = [\n",
    "            item for item in sorted_items\n",
    "            if calculate_iou(current[2], item[2]) < iou_threshold\n",
    "        ]\n",
    "    \n",
    "    return keep\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = detector_image_processor(images=[image], return_tensors=\"pt\")\n",
    "        outputs = detector(**inputs.to(device))\n",
    "        target_sizes = torch.tensor([[image.size[1], image.size[0]]])\n",
    "        results = detector_image_processor.post_process_object_detection(outputs, threshold=0.3, target_sizes=target_sizes)[0]\n",
    "\n",
    "    items = []\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        score = score.item()\n",
    "        label = label.item()\n",
    "        box = [i.item() for i in box]\n",
    "        print(f\"{detector.config.id2label[label]}: {round(score, 3)} at {box}\")\n",
    "        items.append((score, label, box))\n",
    "    \n",
    "    items = non_max_suppression(items)\n",
    "        \n",
    "    original_image = copy.deepcopy(image)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default(size=20)\n",
    "    result = []\n",
    "    for score, label, bbox in items:\n",
    "        box = [round(i, 2) for i in bbox]\n",
    "        result.append((original_image.crop(bbox), detector.config.id2label[label], score))\n",
    "        x, y, x2, y2 = tuple(bbox)\n",
    "        draw.rectangle((x, y, x2, y2), outline=\"green\", width=3)\n",
    "        text_position = (bbox[0], bbox[1] - 25)\n",
    "        draw.text(text_position, f\"{detector.config.id2label[label]} {score:.2f}\", fill=\"red\", font=font)\n",
    "\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.Resize((encoder_config.image_size, encoder_config.image_size)),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=encoder_image_processor.image_mean, std=encoder_image_processor.image_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image_paths = []\n",
    "\n",
    "categories = [d for d in os.listdir('../crawl/kream_thumbnails') if not d.startswith('.')]\n",
    "for category in categories:\n",
    "    fnames = [d for d in os.listdir(f'../crawl/kream_thumbnails/{category}') if not d.startswith('.')]\n",
    "    target_image_paths.extend([f'../crawl/kream_thumbnails/{category}/{f}' for f in fnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThumbnailDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, image_processor):\n",
    "        super(ThumbnailDataset, self).__init__()\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.image_processor = image_processor\n",
    "        self.transform = v2.Compose([\n",
    "            v2.Resize((224, 224)),\n",
    "            v2.ToTensor(),\n",
    "            v2.Normalize(mean=self.image_processor.image_mean, std=self.image_processor.image_std),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image = Image.open(self.image_paths[i]).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "dataset = ThumbnailDataset(target_image_paths, encoder_image_processor)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=128, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_embeddings = []\n",
    "for images in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        embeddings = encoder(images.to(device)).tolist()\n",
    "        target_embeddings.extend(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(query_embeddings, target_embeddings, top_k=5):\n",
    "    test = query_embeddings\n",
    "    target = target_embeddings\n",
    "    \n",
    "    distances = np.linalg.norm(test[:, np.newaxis] - target, axis=2)\n",
    "    \n",
    "    nearest_indices = np.argsort(distances, axis=1)[:, :top_k]\n",
    "    \n",
    "    return nearest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_query_and_results(query_images, target_image_paths, nearest_indices, n_results=5):\n",
    "    n_queries = len(query_images)\n",
    "    fig, axes = plt.subplots(n_queries, n_results + 1, figsize=(3 * (n_results + 1), 3 * n_queries))\n",
    "\n",
    "    if n_queries == 1:\n",
    "        for i, query_image in enumerate(query_images):\n",
    "            axes[0].imshow(query_image)\n",
    "            axes[0].set_title(f\"Query {i+1}\")\n",
    "            axes[0].axis('off')\n",
    "\n",
    "            axes[1].axis('off')\n",
    "            axes[1].axvline(x=0.5, color='black', linewidth=2)\n",
    "            \n",
    "            for j in range(n_results):\n",
    "                result_idx = nearest_indices[i, j]\n",
    "                axes[j+1].imshow(Image.open(target_image_paths[result_idx]).convert('RGB'))\n",
    "                axes[j+1].set_title(f\"Result {j+1}\")\n",
    "                axes[j+1].axis('off')\n",
    "    else: \n",
    "        for i, query_image in enumerate(query_images):\n",
    "            axes[i, 0].imshow(query_image)\n",
    "            axes[i, 0].set_title(f\"Query {i+1}\")\n",
    "            axes[i, 0].axis('off')\n",
    "\n",
    "            axes[i, 1].axis('off')\n",
    "            axes[i, 1].axvline(x=0.5, color='black', linewidth=2)\n",
    "            \n",
    "            for j in range(n_results):\n",
    "                result_idx = nearest_indices[i, j]\n",
    "                axes[i, j+1].imshow(Image.open(target_image_paths[result_idx]).convert('RGB'))\n",
    "                axes[i, j+1].set_title(f\"Result {j+1}\")\n",
    "                axes[i, j+1].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image_fnames = [f for f in os.listdir('../media') if f.startswith('query_image')]\n",
    "query_image_paths = sorted([f'../media/{f}' for f in query_image_fnames])\n",
    "\n",
    "for query_image_path in query_image_paths:\n",
    "    result = detect_objects(query_image_path)\n",
    "    cropped_images = [r[0] for r in result]\n",
    "\n",
    "    cropped_image_tensors = torch.stack([transform(image) for image in cropped_images])\n",
    "    with torch.no_grad():\n",
    "        cropped_image_embeddings = encoder(cropped_image_tensors.to(device)).tolist()\n",
    "        cropped_image_embeddings = np.array(cropped_image_embeddings)\n",
    "\n",
    "    nearest_indices = find_nearest_neighbors(cropped_image_embeddings, target_embeddings)\n",
    "    visualize_query_and_results(cropped_images, target_image_paths, nearest_indices, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion-visual-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
