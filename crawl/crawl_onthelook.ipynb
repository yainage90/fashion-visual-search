{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as v2\n",
    "from transformers import  ConditionalDetrImageProcessor, ConditionalDetrForObjectDetection\n",
    "from transformers import ViTConfig, ViTImageProcessor, ViTModel\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"../object_detection/model_ckpt\"\n",
    "object_detection_image_processor = ConditionalDetrImageProcessor.from_pretrained(ckpt)\n",
    "object_detector = ConditionalDetrForObjectDetection.from_pretrained(ckpt).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x3, y3, x4, y4 = box2\n",
    "\n",
    "    xx1, yy1 = max(x1, x3), max(y1, y3)\n",
    "    xx2, yy2 = min(x2, x4), min(y2, y4)\n",
    "\n",
    "    intersection_area = max(0, xx2 - xx1) * max(0, yy2 - yy1)\n",
    "\n",
    "    box1_area = (x2 - x1) * (y2 - y1)\n",
    "    box2_area = (x4 - x3) * (y4 - y3)\n",
    "\n",
    "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
    "    return iou\n",
    "\n",
    "def non_max_suppression(items, iou_threshold=0.75):\n",
    "    sorted_items = sorted(items, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    keep = []\n",
    "    while sorted_items:\n",
    "        current = sorted_items.pop(0)\n",
    "        keep.append(current)\n",
    "        \n",
    "        sorted_items = [\n",
    "            item for item in sorted_items\n",
    "            if calculate_iou(current[2], item[2]) < iou_threshold\n",
    "        ]\n",
    "    \n",
    "    return keep\n",
    "\n",
    "def detect_objects(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = object_detection_image_processor(images=[image], return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = object_detector(**inputs.to(device))\n",
    "        target_sizes = torch.tensor([[image.size[1], image.size[0]]])\n",
    "        results = object_detection_image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "    items = []\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        box = [i.item() for i in box]\n",
    "        items.append((score.item(), label.item(), box))\n",
    "    \n",
    "    # NMS 적용\n",
    "    items = non_max_suppression(items)\n",
    "        \n",
    "    result = []\n",
    "    for score, label, bbox in items:\n",
    "        box = [round(i, 2) for i in bbox]\n",
    "        result.append((image.crop(bbox), object_detector.config.id2label[label], score))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_labels = ['bag', 'bottom', 'dress', 'hat', 'outer', 'shoes', 'top', 'etc']\n",
    "cls_id2label = {\n",
    "    i: l for i, l in enumerate(cls_labels)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = 'google/vit-base-patch16-224-in21k'\n",
    "config = ViTConfig.from_pretrained(ckpt)\n",
    "vit_image_processor = ViTImageProcessor.from_pretrained(ckpt)\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.Resize((config.image_size, config.image_size)),\n",
    "    v2.ToTensor(),\n",
    "    v2.Normalize(mean=vit_image_processor.image_mean, std=vit_image_processor.image_std),\n",
    "])\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.fc = nn.Linear(config.hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        logits = self.fc(self.vit(x).pooler_output)\n",
    "        return logits\n",
    "\n",
    "classifier = Classifier(num_labels=len(cls_id2label))\n",
    "classifier.load_state_dict(torch.load('../image_encoder/classification_model_ckpt/classifier.pt'))\n",
    "classifier = classifier.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'\n",
    "}\n",
    "def save_images(post_id):\n",
    "    url = f'https://onthelook.co.kr/post/{post_id}'\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 0.5).until(EC.presence_of_element_located((By.CLASS_NAME, 'social_image_box')))\n",
    "\n",
    "    post_image = driver.find_element(By.ID, 'post-image')\n",
    "    post_image_url = post_image.get_attribute('src')\n",
    "    post_image = Image.open(requests.get(post_image_url, stream=True).raw).convert('RGB')\n",
    "\n",
    "    product_items = driver.find_elements(By.CLASS_NAME, 'product-item')\n",
    "\n",
    "    product_images = []\n",
    "    for product_item in product_items:\n",
    "        img_tag = product_item.find_element(By.TAG_NAME, 'img')\n",
    "        product_image_url = img_tag.get_attribute('src').split('?')[0]\n",
    "        product_image = Image.open(requests.get(product_image_url, headers=headers, stream=True).raw).convert('RGB')\n",
    "        product_images.append(product_image)\n",
    "\n",
    "    if not product_items:\n",
    "        return\n",
    "\n",
    "    result = detect_objects(post_image)\n",
    "\n",
    "    detected_labels = []\n",
    "    shoes_count = 0\n",
    "    detected_label_to_image = {}\n",
    "    for cropped_image, label, score in result:\n",
    "        if label == 'shoes':\n",
    "            shoes_count += 1\n",
    "        detected_labels.append(label)\n",
    "        if label in detected_label_to_image:\n",
    "            if score > detected_label_to_image[label][1]:\n",
    "                detected_label_to_image[label] = ((cropped_image, score))\n",
    "        else:\n",
    "            detected_label_to_image[label] = ((cropped_image, score))\n",
    "\n",
    "    for label, (cropped_image, score) in detected_label_to_image.items():\n",
    "        detected_label_to_image[label] = cropped_image\n",
    "\n",
    "    if shoes_count > 2:\n",
    "        return\n",
    "\n",
    "    if len([i for i in detected_labels if i != \"shoes\"]) != len(set([i for i in detected_labels if i != \"shoes\"])):\n",
    "        return\n",
    "\n",
    "    tag_labels = []\n",
    "    tag_label_to_image = {}\n",
    "\n",
    "    for product_image in product_images:\n",
    "        if not (product_image.width >= 200 and product_image.height >= 200):\n",
    "            continue\n",
    "\n",
    "        image_tensor = transform(product_image).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = classifier(image_tensor.unsqueeze(0)).squeeze()\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        indices = probs.argsort(dim=-1, descending=True).tolist()\n",
    "        max_prob = probs[indices[0]].item()\n",
    "        if max_prob < 0.8:\n",
    "            continue\n",
    "\n",
    "        label = cls_id2label[indices[0]]\n",
    "        if label == 'etc':\n",
    "            continue\n",
    "\n",
    "        tag_labels.append(label)\n",
    "        tag_label_to_image[label] = product_image\n",
    "\n",
    "    if not tag_labels:\n",
    "        return\n",
    "\n",
    "    if not len(tag_labels) == len(set(tag_labels)):\n",
    "        return\n",
    "\n",
    "    for label in tag_labels:\n",
    "        anchor_image = detected_label_to_image.get(label)\n",
    "        if not anchor_image:\n",
    "            return\n",
    "        positive_image = tag_label_to_image[label]\n",
    "\n",
    "        save_dir = f'./onthelook_anchor_positive_images/{label}/{post_id}'\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        anchor_image_path = f'{save_dir}/anchor.jpg'\n",
    "        positive_image_path = f'{save_dir}/positive.jpg'\n",
    "\n",
    "        anchor_image.save(anchor_image_path)\n",
    "        positive_image.save(positive_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_post_id = 1\n",
    "prev_dirs = sorted([d for d in os.listdir('./') if d.startswith('onthelook_dataset')])\n",
    "if prev_dirs:\n",
    "    prev_dataset = load_from_disk(prev_dirs[-1])\n",
    "    start_post_id = max(int(i) for i in prev_dataset['post_id']) + 1\n",
    "\n",
    "print(f'start_post_id: {start_post_id}')\n",
    "\n",
    "for post_id in range(start_post_id, 211727):\n",
    "    try:\n",
    "        save_images(post_id)\n",
    "    except Exception:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fashion-visual-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
